# -*- coding: utf-8 -*-
"""NLP_Resume_parser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g6W-s71K6mMTaCcI4kR8Jmoraqfa1QqQ
"""

!pip install spacy_transformers
!pip install -U spacy

import spacy
from spacy.tokens import DocBin
from tqdm import tqdm
import json

spacy.__version__

!nvidia-smi

!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git

cv_data=json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))

len(cv_data)

!pip install --upgrade pip

!pip install tensorflow-gpu==2.8.0

!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/CV-Parsing-using-Spacy-3/data/training/config.cfg

cv_data

from spacy.tokens import DocBin
from tqdm import tqdm

def get_spacy_doc(file, data):
    nlp = spacy.blank('en')
    db = DocBin()

    for text, annot in tqdm(data):
        doc = nlp.make_doc(text)
        annot = annot['entities']
        ents = []
        entity_indices = []

        for start, end, label in annot:
            skip_entity = False
            for idx in range(start, end):
                if idx in entity_indices:
                    skip_entity = True
                    break
            if skip_entity:
                continue

            entity_indices.extend(range(start, end))

            try:
                span = doc.char_span(start, end, label=label, alignment_mode='strict')
            except:
                continue

            if span is None:
                err_data = f"{[start, end]}   {text}\n"
                file.write(err_data)
            else:
                ents.append(span)

        try:
            doc.ents = ents
            db.add(doc)
        except:
            pass

    return db

from sklearn.model_selection import train_test_split
train,test=train_test_split(cv_data,test_size=0.3)

len(train),len(test)

file=open('error.txt','w')
db=get_spacy_doc(file,train)
db.to_disk('train_data.spacy')

db=get_spacy_doc(file,test)
db.to_disk('test_data.spacy')

file.close()

len(db.tokens)

!python -m spacy train /content/CV-Parsing-using-Spacy-3/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0

nlp=spacy.load('/content/output/model-best')

doc=nlp('my name is neerja kumar.I worked at microsoft.I have 2 years of experience')
for ent in doc.ents:
  print(ent.text,"->>>>>>>>>>>>",ent.label_)

!pip install PyMuPDF

import sys, fitz

fname = '/content/Sheeba manukonda (1) (1).pdf'
doc=fitz.open(fname)

text=" "
for page in doc:
  text=text+str(page.get_text())

text=text.strip()

text=' '.join(text.split())

text

doc=nlp(text)
for ent in doc.ents:
  print(ent.text,"->>>>>>>>>>>>",ent.label_)



